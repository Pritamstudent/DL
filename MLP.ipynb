{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \n",
    "  def __init__(self):\n",
    "    self.w = None\n",
    "    self.b = None\n",
    "    \n",
    "  def perceptron(self, x):\n",
    "    return np.dot(x, self.w.T) + self.b\n",
    "  \n",
    "  def sigmoid(self, x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "  \n",
    "  def grad_w_mse(self, x, y):\n",
    "    y_pred = self.sigmoid(self.perceptron(x))\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred) * x\n",
    "  \n",
    "  def grad_b_mse(self, x, y):\n",
    "    y_pred = self.sigmoid(self.perceptron(x))\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred)\n",
    "  \n",
    "  def grad_w_ce(self, x, y):\n",
    "    y_pred = self.sigmoid(self.perceptron(x))\n",
    "    if y == 0:\n",
    "      return y_pred * x\n",
    "    elif y == 1:\n",
    "      return -1 * (1 - y_pred) * x\n",
    "    else:\n",
    "      raise ValueError(\"y should be 0 or 1\")\n",
    "    \n",
    "  def grad_b_ce(self, x, y):\n",
    "    y_pred = self.sigmoid(self.perceptron(x))\n",
    "    if y == 0:\n",
    "      return y_pred \n",
    "    elif y == 1:\n",
    "      return -1 * (1 - y_pred)\n",
    "    else:\n",
    "      raise ValueError(\"y should be 0 or 1\")\n",
    "  \n",
    "  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn=\"mse\", display_loss=False):\n",
    "    \n",
    "    # initialise w, b\n",
    "    if initialise:\n",
    "      self.w = np.random.randn(1, X.shape[1])\n",
    "      self.b = 0\n",
    "      \n",
    "    if display_loss:\n",
    "      loss = {}\n",
    "    \n",
    "    for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
    "      dw = 0\n",
    "      db = 0\n",
    "      for x, y in zip(X, Y):\n",
    "        if loss_fn == \"mse\":\n",
    "          dw += self.grad_w_mse(x, y)\n",
    "          db += self.grad_b_mse(x, y) \n",
    "        elif loss_fn == \"ce\":\n",
    "          dw += self.grad_w_ce(x, y)\n",
    "          db += self.grad_b_ce(x, y)\n",
    "          \n",
    "      m = X.shape[1]  \n",
    "      self.w -= learning_rate * dw/m\n",
    "      self.b -= learning_rate * db/m\n",
    "      \n",
    "      if display_loss:\n",
    "        Y_pred = self.sigmoid(self.perceptron(X))\n",
    "        if loss_fn == \"mse\":\n",
    "          loss[i] = mean_squared_error(Y, Y_pred)\n",
    "        elif loss_fn == \"ce\":\n",
    "          loss[i] = log_loss(Y, Y_pred)\n",
    "    \n",
    "    if display_loss:\n",
    "      plt.plot(loss.values())\n",
    "      plt.xlabel('Epochs')\n",
    "      if loss_fn == \"mse\":\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "      elif loss_fn == \"ce\":\n",
    "        plt.ylabel('Log Loss')\n",
    "      plt.show()\n",
    "      \n",
    "  def predict(self, X):\n",
    "    Y_pred = []\n",
    "    for x in X:\n",
    "      y_pred = self.sigmoid(self.perceptron(x))\n",
    "      Y_pred.append(y_pred)\n",
    "    return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GOD WORLD\\AppData\\Local\\Temp\\ipykernel_17164\\1894556098.py:49: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c9400573a34dc8bc147bc0f90724ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_df = pd.read_csv(r'dataset/train.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "titanic_df = titanic_df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']].dropna()\n",
    "X = titanic_df.drop('Survived', axis=1).values\n",
    "y = titanic_df['Survived'].values.reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SigmoidNeuron class\n",
    "sn = SigmoidNeuron()\n",
    "\n",
    "# Train the model\n",
    "sn.fit(X_train, y_train, epochs=1000, learning_rate=0.1, initialise=True, loss_fn=\"mse\", display_loss=True)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred = sn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Instantiate the MLP model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m mlp_model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    144\u001b[0m mlp_model\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[1;34m(self, X, Y, X_val, Y_val, L, N_l)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y,X_val,Y_val,L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,N_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X,np\u001b[38;5;241m.\u001b[39mones((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[Y\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Assuming binary classification\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_val,np\u001b[38;5;241m.\u001b[39mones((X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[Y_val\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Assuming binary classification\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = np.squeeze(np.eye(2)[Y.astype(np.int).reshape(-1)])  # Assuming binary classification\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(2)[Y_val.astype(np.int).reshape(-1)])  # Assuming binary classification\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes = np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic_data = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Preprocess your data\n",
    "titanic_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "titanic_data = pd.get_dummies(titanic_data, columns=['Sex'], drop_first=True)\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].median(), inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = titanic_data.drop('Survived', axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the MLP model\n",
    "mlp_model = MLP(X_train, y_train, X_val, y_val, L=1, N_l=128)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.train(batch_size=8, epochs=25, lr=1.0)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = mlp_model.evaluate(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SigmoidNeuron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Instantiate the SigmoidNeuron class\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m sn \u001b[38;5;241m=\u001b[39m \u001b[43mSigmoidNeuron\u001b[49m()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     39\u001b[0m sn\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, initialise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, loss_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, display_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SigmoidNeuron' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset/adult.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "# Assuming '?' represents missing values, replace them with NaN\n",
    "data.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Split the data into features (X) and target variable (Y)\n",
    "X = data.drop(columns=['income'])\n",
    "Y = np.array(data['income'])\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (optional but recommended for better performance)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the SigmoidNeuron class\n",
    "sn = SigmoidNeuron()\n",
    "\n",
    "# Train the model\n",
    "sn.fit(X_train_scaled, y_train, epochs=1000, learning_rate=0.1, initialise=True, loss_fn=\"mse\", display_loss=True)\n",
    "\n",
    "# Make predictions\n",
    "predictions = sn.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to binary (>= 0.5 is considered as 1, < 0.5 is considered as 0)\n",
    "binary_predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", binary_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Instantiate the MLP model\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m mlp_model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    144\u001b[0m mlp_model\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[1;34m(self, X, Y, X_val, Y_val, L, N_l)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y,X_val,Y_val,L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,N_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X,np\u001b[38;5;241m.\u001b[39mones((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Updated here\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_val,np\u001b[38;5;241m.\u001b[39mones((X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[Y_val\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Updated here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self,X,Y,X_val,Y_val,L=1,N_l=128):\n",
    "        self.X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.Y = np.squeeze(np.eye(2)[Y.astype(int).reshape(-1)])  # Updated here\n",
    "        self.X_val = np.concatenate((X_val,np.ones((X_val.shape[0],1))),axis=1)\n",
    "        self.Y_val = np.squeeze(np.eye(2)[Y_val.astype(int).reshape(-1)])  # Updated here\n",
    "        self.L = L\n",
    "        self.N_l = N_l\n",
    "        self.n_samples = self.X.shape[0]\n",
    "        self.layer_sizes = np.array([self.X.shape[1]]+[N_l]*L+[self.Y.shape[1]]) \n",
    "        self.__init_weights()\n",
    "        self.train_loss = list()\n",
    "        self.train_acc = list()\n",
    "        self.val_loss = list()\n",
    "        self.val_acc = list()\n",
    "        self.train_time = list()\n",
    "        self.tot_time = list()\n",
    "        self.metrics = [self.train_loss,self.train_acc,self.val_loss,self.val_acc,self.train_time,self.tot_time]\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "    def __softmax(self,x):\n",
    "        exponent = np.exp(x)\n",
    "        return exponent/exponent.sum(axis=1,keepdims=True)\n",
    "    \n",
    "    def __loss(self,y_pred,y):\n",
    "        return ((-np.log(y_pred))*y).sum(axis=1).mean()\n",
    "    \n",
    "    def __accuracy(self,y_pred,y):  \n",
    "        return np.all(y_pred==y,axis=1).mean()\n",
    "    \n",
    "    def __sigmoid_prime(self,h):\n",
    "        return h*(1-h)\n",
    "    \n",
    "    def __to_categorical(self,x):  \n",
    "        categorical = np.zeros((x.shape[0],self.Y.shape[1]))\n",
    "        categorical[np.arange(x.shape[0]),x.argmax(axis=1)] = 1\n",
    "        return categorical\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        self.weights = list()\n",
    "        for i in range(self.layer_sizes.shape[0]-1):\n",
    "            self.weights.append(np.random.uniform(-1,1,size=[self.layer_sizes[i],self.layer_sizes[i+1]]))\n",
    "        self.weights = np.asarray(self.weights)\n",
    "    \n",
    "    def __init_layers(self,batch_size):\n",
    "        self.__h = [np.empty((batch_size,layer)) for layer in self.layer_sizes]\n",
    "    \n",
    "    def __feed_forward(self,batch):\n",
    "        h_l = batch\n",
    "        self.__h[0] = h_l\n",
    "        for i,weights in enumerate(self.weights):\n",
    "            h_l = self.__sigmoid(h_l.dot(weights))\n",
    "            self.__h[i+1]=h_l\n",
    "        self.__out = self.__softmax(self.__h[-1])\n",
    "    \n",
    "    def __back_prop(self,batch_y):\n",
    "        delta_t = (self.__out - batch_y)*self.__sigmoid_prime(self.__h[-1])\n",
    "        for i in range(1,len(self.weights)+1):\n",
    "            self.weights[-i]-=self.lr*(self.__h[-i-1].T.dot(delta_t))/self.batch_size\n",
    "            delta_t = self.__sigmoid_prime(self.__h[-i-1])*(delta_t.dot(self.weights[-i].T))\n",
    "            \n",
    "    def predict(self,X):\n",
    "        X = np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "        self.__init_layers(X.shape[0])\n",
    "        self.__feed_forward(X)\n",
    "        return self.__to_categorical(self.__out)\n",
    "    \n",
    "    def evaluate(self,X,Y):\n",
    "        prediction = self.predict(X)\n",
    "        return self.__accuracy(prediction,Y)\n",
    "        \n",
    "    def train(self,batch_size=8,epochs=25,lr=1.0):\n",
    "        self.lr = lr\n",
    "        self.batch_size=batch_size\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            self.__init_layers(self.batch_size)\n",
    "            shuffle = np.random.permutation(self.n_samples)\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            X_batches = np.array_split(self.X[shuffle],self.n_samples/self.batch_size)\n",
    "            Y_batches = np.array_split(self.Y[shuffle],self.n_samples/self.batch_size)\n",
    "            for batch_x,batch_y in zip(X_batches,Y_batches):\n",
    "                self.__feed_forward(batch_x)  \n",
    "                train_loss += self.__loss(self.__out,batch_y)\n",
    "                train_acc += self.__accuracy(self.__to_categorical(self.__out),batch_y)\n",
    "                self.__back_prop(batch_y)\n",
    "                \n",
    "            train_loss = (train_loss/len(X_batches))\n",
    "            train_acc = (train_acc/len(X_batches))\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.train_acc.append(train_acc)\n",
    "            \n",
    "            train_time = round(time.time()-start,3)\n",
    "            self.train_time.append(train_time)\n",
    "            \n",
    "            self.__init_layers(self.X_val.shape[0])\n",
    "            self.__feed_forward(self.X_val)\n",
    "            val_loss = self.__loss(self.__out,self.Y_val)\n",
    "            val_acc = self.__accuracy(self.__to_categorical(self.__out),self.Y_val)\n",
    "            self.val_loss.append(val_loss)\n",
    "            self.val_acc.append(val_acc)\n",
    "            \n",
    "            tot_time = round(time.time()-start,3)\n",
    "            self.tot_time.append(tot_time)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: loss = {train_loss.round(3)} | acc = {train_acc.round(3)} | val_loss = {val_loss.round(3)} | val_acc = {val_acc.round(3)} | train_time = {train_time} | tot_time = {tot_time}\")\n",
    "\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic_data = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Preprocess your data\n",
    "titanic_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n",
    "titanic_data = pd.get_dummies(titanic_data, columns=['Sex'], drop_first=True)\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
    "titanic_data['Fare'].fillna(titanic_data['Fare'].median(), inplace=True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = titanic_data.drop('Survived', axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the MLP model\n",
    "mlp_model = MLP(X_train, y_train, X_val, y_val, L=1, N_l=128)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.train(batch_size=8, epochs=25, lr=1.0)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = mlp_model.evaluate(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Step 4: Instantiate the MLP model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m mlp_model \u001b[38;5;241m=\u001b[39m \u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m mlp_model\u001b[38;5;241m.\u001b[39mtrain(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mMLP.__init__\u001b[1;34m(self, X, Y, X_val, Y_val, L, N_l)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,X,Y,X_val,Y_val,L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,N_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X,np\u001b[38;5;241m.\u001b[39mones((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Updated here\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X_val,np\u001b[38;5;241m.\u001b[39mones((X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m))),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m)[Y_val\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])  \u001b[38;5;66;03m# Updated here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Step 1: Read the dataset\n",
    "data = pd.read_csv(\"dataset\\indian_liver_patient.csv\")  # Replace \"your_dataset.csv\" with the path to your dataset\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Encode categorical variable \"Gender\" using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=[\"Gender\"])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=[\"Dataset\"])  # Features\n",
    "y = data[\"Dataset\"]  # Labels\n",
    "\n",
    "# Normalize numerical features if necessary\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Normalize numerical features\n",
    "\n",
    "# Step 3: Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Instantiate the MLP model\n",
    "mlp_model = MLP(X_train, y_train, X_val, y_val, L=1, N_l=128)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.train(batch_size=8, epochs=25, lr=1.0)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = mlp_model.evaluate(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
